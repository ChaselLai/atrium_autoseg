{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import math\n",
    "import SimpleITK as sitk\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from keras import utils\n",
    "from keras.preprocessing import image as keras_image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UNet(object):\n",
    "    def __init__(self, img_row, img_col):\n",
    "        self.img_row = img_row\n",
    "        self.img_col = img_col\n",
    "        \n",
    "    def unet(self):\n",
    "        inputs = Input((self.img_rows, self.img_cols,1))\n",
    "        #first convolutional layer - output 64 filters, kernel size 3*3, use relu activation function and he normal kernel\n",
    "        #this draws samples from a truncated normal distribution centered on 0 with stdec = sqrt(2/# of input units in weight tensor)\n",
    "        #used same padding to make sure output has the same length as input\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        #then we do a maxpooling \n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        print (\"pool1 shape:\",pool1.shape)\n",
    "        \n",
    "        #then we takes in the pooled output and feed into another convolutional layers with 128 filters\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool2 shape:\",pool2.shape)\n",
    "        \n",
    "        #then we do another convolutional layer with 256 filters\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool3 shape:\",pool3.shape)\n",
    "        \n",
    "        #in order to prevent vanishing gradient problem, we apply dropout in the 4th convolutional layer\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        print (\"conv4 shape:\",conv3.shape)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        print (\"conv4 shape:\",conv3.shape)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool4 shape:\",pool3.shape)\n",
    "        \n",
    "        #same for 5th convolutional layer\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        drop5 = Dropout(0.5)(conv5)\n",
    "        \n",
    "        #we then start upconv output of conv 5 (2*2 filter) -> then copy 512 filter conv layer and merge\n",
    "        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "        merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\n",
    "        #then we do convolution again\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        \n",
    "        #we then upconv output of conv 6 (2*2 filter)-> then copy 256 filter conv layer and merge -> conv 2 times\n",
    "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "        merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "        \n",
    "        #upconv output of conv7 (2*2 filter) -> copy 128 filter conv layer and merge -> conv 2 tiems\n",
    "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "        merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        \n",
    "        #upconv output of conv8 (2*2 filter) -> copy 64 filter conv layer and merge -> conv 2 times\n",
    "        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "        merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "        \n",
    "        model = Model(input = inputs, output = conv10)\n",
    "\n",
    "        model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_nrrd(full_path_filename):\n",
    "\n",
    "\tdata = sitk.ReadImage(full_path_filename)\n",
    "\tdata = sitk.Cast(sitk.RescaleIntensity(data),sitk.sitkUInt8)\n",
    "\tdata = sitk.GetArrayFromImage(data)\n",
    "\n",
    "\treturn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#decode mask \t\n",
    "def run_length_decoding(run_lengths,img):\n",
    "    \n",
    "    h,w = img.shape\n",
    "    mask = np.zeros(h*w)\n",
    "    if run_lengths == '[\\n]':\n",
    "        pass\n",
    "    else:\n",
    "        run_lengths_s = run_lengths[0].split()\n",
    "        #print(run_lengths_s)\n",
    "        for i in range(len(run_lengths_s)):\n",
    "            #even number is index and odd number is # of consecutive tags\n",
    "            if i%2 == 0:\n",
    "                #print(i)\n",
    "                mask[(int(run_lengths_s[i])-1):(int(run_lengths_s[i])+int(run_lengths_s[i+1])-1)] = 1\n",
    "        mask = mask.reshape((h,w)).T\n",
    "        \n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = '/Users/qinwenhuang/Documents/AtriaSeg_2018_training/train_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = {}\n",
    "with open (train_file,'r') as f:\n",
    "    for ind,line in enumerate(f):\n",
    "        if ind > 0:\n",
    "            line = line.split(',')\n",
    "            train_labels[(line[0])]=line[1:]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomShiftScaleRotate(image, mask,\n",
    "                           shift_limit=(-0.0625, 0.0625),\n",
    "                           scale_limit=(-0.1, 0.1),\n",
    "                           rotate_limit=(-45, 45), aspect_limit=(0, 0),\n",
    "                           borderMode=cv2.BORDER_CONSTANT, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        height, width, channel = image.shape\n",
    "\n",
    "        angle = np.random.uniform(rotate_limit[0], rotate_limit[1])  # degree\n",
    "        scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])\n",
    "        aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])\n",
    "        sx = scale * aspect / (aspect ** 0.5)\n",
    "        sy = scale / (aspect ** 0.5)\n",
    "        dx = round(np.random.uniform(shift_limit[0], shift_limit[1]) * width)\n",
    "        dy = round(np.random.uniform(shift_limit[0], shift_limit[1]) * height)\n",
    "\n",
    "        cc = np.math.cos(angle / 180 * np.math.pi) * sx\n",
    "        ss = np.math.sin(angle / 180 * np.math.pi) * sy\n",
    "        rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n",
    "\n",
    "        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])\n",
    "        box1 = box0 - np.array([width / 2, height / 2])\n",
    "        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])\n",
    "\n",
    "        box0 = box0.astype(np.float32)\n",
    "        box1 = box1.astype(np.float32)\n",
    "        mat = cv2.getPerspectiveTransform(box0, box1)\n",
    "        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                    borderValue=(\n",
    "                                        0, 0,\n",
    "                                        0,))\n",
    "        mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                   borderValue=(\n",
    "                                       0, 0,\n",
    "                                       0,))\n",
    "\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomHorizontalFlip(image, mask, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = cv2.flip(image, 1)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataLoading(object):\n",
    "    \"\"\"\n",
    "    data directory structure\n",
    "    TrainingSet/\n",
    "    each patient/\n",
    "    mask.nrrd, original.nrrd\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, mask_dict):\n",
    "        self.directory = directory\n",
    "        self.mask_dict = mask_dict\n",
    "        self.mris = []\n",
    "        self.mri_names = []\n",
    "        self.masks = []\n",
    "        #self.load_images()\n",
    "        #self.load_masks()\n",
    "    def load_images(self):\n",
    "        \"\"\"\n",
    "        load all images from training set \n",
    "        go through subdirectories and get lgemri.nrrd, which represents patients original mri images\n",
    "        uses load_nrrd function \n",
    "        retun mri matrices and mri names\n",
    "        \"\"\"\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.directory, topdown = False):\n",
    "            for name in files:\n",
    "                if name == 'lgemri.nrrd':\n",
    "                    #print('yes')\n",
    "                    patient_name = root[-20:]+'_Slice_'\n",
    "                    full_name = os.path.join(root,name)\n",
    "                    single_patient_image = load_nrrd(full_name)\n",
    "                    num_of_slices = single_patient_image.shape[0]\n",
    "                    for i in range(num_of_slices):\n",
    "                        self.mris.append(single_patient_image[i])\n",
    "                        self.mri_names.append(patient_name+str(i))\n",
    "        return self.mris, self.mri_names \n",
    "    def load_masks(self):\n",
    "        \"\"\"\n",
    "        covert all masks in rle to matrix format \n",
    "        return matrix format mask list\n",
    "        \"\"\"\n",
    "        #self.masks = []\n",
    "        for idx,name in enumerate(self.mri_names):\n",
    "            img = self.mris[idx]\n",
    "            encode_cav = train_labels[name]\n",
    "            #print(encode_cav)\n",
    "            output_mask = run_length_decoding(encode_cav,img)\n",
    "            self.masks.append(output_mask)\n",
    "        return self.masks\n",
    "            \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = dataLoading(topdir, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_images(directory, mask):\n",
    "    all_data = dataLoading(directory, mask)\n",
    "    image, image_name = all_data.load_images()\n",
    "    masks = all_data.load_masks()\n",
    "    return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topdir = '/Users/qinwenhuang/Documents/AtriaSeg_2018_training/Training Set'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different loss functions (dice, jaccard coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to compute dice coefficient\n",
    "def soft_sorensen_dice(y_true, y_pred, axis=None, smooth=1):\n",
    "    intersect = K.sum(y_true * y_pred, axis=axis)\n",
    "    area_true = K.sum(y_true, axis=axis)\n",
    "    area_pred = K.sum(y_pred, axis=axis)\n",
    "    return (2 * intersect + smooth) / (area_true + area_pred + smooth)\n",
    "    \n",
    "def hard_sorensen_dice(y_true, y_pred, axis=None, smooth=1):\n",
    "    y_true_int = K.round(y_true)\n",
    "    y_pred_int = K.round(y_pred)\n",
    "    return soft_sorensen_dice(y_true_int, y_pred_int, axis, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorensen_dice_loss(y_true, y_pred, weights):\n",
    "    # Input tensors have shape (batch_size, height, width, classes)\n",
    "    # must input list of weights with length equal to number of classes\n",
    "    #\n",
    "    # Ex: for simple binary classification, with the 0th mask\n",
    "    # corresponding to the background and the 1st mask corresponding\n",
    "    # to the object of interest, we set weights = [0, 1]\n",
    "    batch_dice_coefs = soft_sorensen_dice(y_true, y_pred, axis=[1, 2])\n",
    "    dice_coefs = K.mean(batch_dice_coefs, axis=0)\n",
    "    w = K.constant(weights) / sum(weights)\n",
    "    return 1 - K.sum(w * dice_coefs)\n",
    "\n",
    "def soft_jaccard(y_true, y_pred, axis=None, smooth=1):\n",
    "    intersect = K.sum(y_true * y_pred, axis=axis)\n",
    "    area_true = K.sum(y_true, axis=axis)\n",
    "    area_pred = K.sum(y_pred, axis=axis)\n",
    "    union = area_true + area_pred - intersection\n",
    "    return (intersect + smooth) / (union + smooth)\n",
    "\n",
    "def hard_jaccard(y_true, y_pred, axis=None, smooth=1):\n",
    "    y_true_int = K.round(y_true)\n",
    "    y_pred_int = K.round(y_pred)\n",
    "    return soft_jaccard(y_true_int, y_pred_int, axis, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_loss(y_true, y_pred, weights):\n",
    "    batch_jaccard_coefs = soft_jaccard(y_true, y_pred, axis=[1, 2])\n",
    "    jaccard_coefs = K.mean(batch_jaccard_coefs, axis=0)\n",
    "    w = K.constant(weights) / sum(weights)\n",
    "    return 1 - K.sum(w * jaccard_coefs)\n",
    "\n",
    "def weighted_categorical_crossentropy(y_true, y_pred, weights, epsilon=1e-8):\n",
    "    ndim = K.ndim(y_pred)\n",
    "    ncategory = K.int_shape(y_pred)[-1]\n",
    "    # scale predictions so class probabilities of each pixel sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=(ndim-1), keepdims=True)\n",
    "    y_pred = K.clip(y_pred, epsilon, 1-epsilon)\n",
    "    w = K.constant(weights) * (ncategory / sum(weights))\n",
    "    # first, average over all axis except classes\n",
    "    cross_entropies = -K.mean(y_true * K.log(y_pred), axis=tuple(range(ndim-1)))\n",
    "    return K.sum(w * cross_entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elastic_transform(image, alpha, sigma, mode, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003].\n",
    "       Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\"\n",
    "    \"\"\"\n",
    "    assert len(image.shape)==2\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "    indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n",
    "    \n",
    "    return map_coordinates(image, indices, order=1, mode = mode).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create iterator for better data iteration - this is for loading data to Neural Nets  \n",
    "class Iterator(object):\n",
    "    def __init__(self, images, masks, batch_size,\n",
    "                 shuffle=True,\n",
    "                 rotation_range=90,\n",
    "                 width_shift_range=0.1,\n",
    "                 height_shift_range=0.1,\n",
    "                 shear_range=0.1,\n",
    "                 zoom_range=0.01,\n",
    "                 fill_mode='nearest',\n",
    "                 alpha=500,\n",
    "                 sigma=20):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        augment_options = {\n",
    "            'rotation_range': rotation_range,\n",
    "            'width_shift_range': width_shift_range,\n",
    "            'height_shift_range': height_shift_range,\n",
    "            'shear_range': shear_range,\n",
    "            'zoom_range': zoom_range,\n",
    "            'fill_mode': fill_mode,\n",
    "        }\n",
    "        self.idg = ImageDataGenerator(**augment_options)\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.fill_mode = fill_mode\n",
    "        self.i = 0\n",
    "        self.index = np.arange(len(images))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        # compute how many images to output in this batch\n",
    "        start = self.i\n",
    "        end = min(start + self.batch_size, len(self.images))\n",
    "\n",
    "        augmented_images = []\n",
    "        augmented_masks = []\n",
    "        for n in self.index[start:end]:\n",
    "            image = self.images[n]\n",
    "            mask = self.masks[n]\n",
    "\n",
    "            h,w = image.shape\n",
    "\n",
    "            # stack image + mask together to simultaneously augment\n",
    "            stacked = np.concatenate((image, mask), axis=2)\n",
    "\n",
    "            # apply simple affine transforms first using Keras\n",
    "            augmented = self.idg.random_transform(stacked)\n",
    "\n",
    "            # maybe apply elastic deformation\n",
    "            if self.alpha != 0 and self.sigma != 0:\n",
    "                augmented = elastic_trnasform(\n",
    "                    augmented, self.alpha, self.sigma, self.fill_mode)\n",
    "\n",
    "            # split image and mask back apart\n",
    "            augmented_image = augmented[:,:,0]\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_mask = np.round(augmented[:,:,1])\n",
    "            augmented_masks.append(augmented_mask)\n",
    "\n",
    "        self.i += self.batch_size\n",
    "        if self.i >= len(self.images):\n",
    "            self.i = 0\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.index)\n",
    "\n",
    "        return np.asarray(augmented_images), np.asarray(augmented_masks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x, epsilon=1e-7, axis=1):\n",
    "    \n",
    "    x -= np.mean(x, axis=axis, keepdims=True)\n",
    "    x /= np.std(x, axis=axis, keepdims=True) + epsilon\n",
    "\n",
    "def create_generators(data_dir, batch_size, validation_split=0.0, mask='both',\n",
    "                      shuffle_train_val=True, shuffle=True, seed=None,\n",
    "                      normalize_images=True, augment_training=False,\n",
    "                      augment_validation=False, augmentation_args={}):\n",
    "    images, masks = load_images(data_dir, mask)\n",
    "\n",
    "    # before: type(masks) = uint8 and type(images) = uint8\n",
    "    # convert images to double-precision\n",
    "    images = images.astype('float64')\n",
    "\n",
    "    # maybe normalize image\n",
    "    if normalize_images:\n",
    "        normalize(images, axis=1)\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if shuffle_train_val:\n",
    "        # shuffle images and masks in parallel\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(images)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(masks)\n",
    "\n",
    "    # split out last %(validation_split) of images as validation set\n",
    "    split_index = int((1-validation_split) * len(images))\n",
    "\n",
    "    if augment_training:\n",
    "        train_generator = Iterator(\n",
    "            images[:split_index], masks[:split_index],\n",
    "            batch_size, shuffle=shuffle, **augmentation_args)\n",
    "    else:\n",
    "        idg = ImageDataGenerator()\n",
    "        train_generator = idg.flow(images[:split_index], masks[:split_index],\n",
    "                                   batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    train_steps_per_epoch = ceil(split_index / batch_size)\n",
    "\n",
    "    if validation_split > 0.0:\n",
    "        if augment_validation:\n",
    "            val_generator = Iterator(\n",
    "                images[split_index:], masks[split_index:],\n",
    "                batch_size, shuffle=shuffle, **augmentation_args)\n",
    "        else:\n",
    "            idg = ImageDataGenerator()\n",
    "            val_generator = idg.flow(images[split_index:], masks[split_index:],\n",
    "                                     batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        val_generator = None\n",
    "\n",
    "    val_steps_per_epoch = ceil((len(images) - split_index) / batch_size)\n",
    "\n",
    "    return (train_generator, train_steps_per_epoch,\n",
    "            val_generator, val_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
