{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qinwen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import math\n",
    "import SimpleITK as sitk\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from keras import utils\n",
    "from keras.preprocessing import image as keras_image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6740274298041457869\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5974523904\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16625992112684488620\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpu:0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose\n",
    "from keras.layers import MaxPooling2D, Cropping2D, Concatenate\n",
    "from keras.layers import Lambda, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UNet(object):\n",
    "    def __init__(self, img_row, img_col):\n",
    "        self.img_row = img_row\n",
    "        self.img_col = img_col\n",
    "        \n",
    "    def unet(self):\n",
    "        inputs = Input((self.img_row, self.img_col,1))\n",
    "        #first convolutional layer - output 64 filters, kernel size 3*3, use relu activation function and he normal kernel\n",
    "        #this draws samples from a truncated normal distribution centered on 0 with stdec = sqrt(2/# of input units in weight tensor)\n",
    "        #used same padding to make sure output has the same length as input\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "        print (\"conv1 shape:\",conv1.shape)\n",
    "        #then we do a maxpooling \n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        print (\"pool1 shape:\",pool1.shape)\n",
    "        \n",
    "        #then we takes in the pooled output and feed into another convolutional layers with 128 filters\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "        print (\"conv2 shape:\",conv2.shape)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool2 shape:\",pool2.shape)\n",
    "        \n",
    "        #then we do another convolutional layer with 256 filters\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "        print (\"conv3 shape:\",conv3.shape)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool3 shape:\",pool3.shape)\n",
    "        \n",
    "        #in order to prevent vanishing gradient problem, we apply dropout in the 4th convolutional layer\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "        print (\"conv4 shape:\",conv3.shape)\n",
    "        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "        print (\"conv4 shape:\",conv3.shape)\n",
    "        drop4 = Dropout(0.5)(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        print (\"pool4 shape:\",pool3.shape)\n",
    "        \n",
    "        #same for 5th convolutional layer\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "        drop5 = Dropout(0.5)(conv5)\n",
    "        \n",
    "        #we then start upconv output of conv 5 (2*2 filter) -> then copy 512 filter conv layer and merge\n",
    "        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "        merge6 = merge([drop4,up6], mode = 'concat', concat_axis = 3)\n",
    "        #then we do convolution again\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "        \n",
    "        #we then upconv output of conv 6 (2*2 filter)-> then copy 256 filter conv layer and merge -> conv 2 times\n",
    "        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "        merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "        \n",
    "        #upconv output of conv7 (2*2 filter) -> copy 128 filter conv layer and merge -> conv 2 tiems\n",
    "        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "        merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "        \n",
    "        #upconv output of conv8 (2*2 filter) -> copy 64 filter conv layer and merge -> conv 2 times\n",
    "        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "        merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "        \n",
    "        model = Model(input = inputs, output = conv10)\n",
    "\n",
    "        model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Code For UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling_block(input_tensor, filters, padding='same',\n",
    "                       batchnorm=False, dropout=0.0):\n",
    "    batch, height, width, _ = K.int_shape(input_tensor)\n",
    "    #assert height % 2 == 0\n",
    "    #assert width % 2 == 0\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=(3,3), padding=padding)(input_tensor)\n",
    "    x = BatchNormalization()(x) if batchnorm else x\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x) if dropout > 0 else x\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=(3,3), padding=padding)(x)\n",
    "    x = BatchNormalization()(x) if batchnorm else x\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x) if dropout > 0 else x\n",
    "\n",
    "    return MaxPooling2D(pool_size=(2,2))(x), x\n",
    "\n",
    "def upsampling_block(input_tensor, skip_tensor, filters, padding='same',\n",
    "                     batchnorm=False, dropout=0.0):\n",
    "    x = Conv2DTranspose(filters, kernel_size=(2,2), strides=(2,2))(input_tensor)\n",
    "\n",
    "    # compute amount of cropping needed for skip_tensor\n",
    "    _, x_height, x_width, _ = K.int_shape(x)\n",
    "    _, s_height, s_width, _ = K.int_shape(skip_tensor)\n",
    "    h_crop = s_height - x_height\n",
    "    w_crop = s_width - x_width\n",
    "    assert h_crop >= 0\n",
    "    assert w_crop >= 0\n",
    "    if h_crop == 0 and w_crop == 0:\n",
    "        y = skip_tensor\n",
    "    else:\n",
    "        cropping = ((h_crop//2, h_crop - h_crop//2), (w_crop//2, w_crop - w_crop//2))\n",
    "        y = Cropping2D(cropping=cropping)(skip_tensor)\n",
    "\n",
    "    x = Concatenate()([x, y])\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=(3,3), padding=padding)(x)\n",
    "    x = BatchNormalization()(x) if batchnorm else x\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x) if dropout > 0 else x\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=(3,3), padding=padding)(x)\n",
    "    x = BatchNormalization()(x) if batchnorm else x\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x) if dropout > 0 else x\n",
    "\n",
    "    return x\n",
    "\n",
    "def unet(height, width, channels, classes, features=64, depth=4,\n",
    "         temperature=1.0, padding='same', batchnorm=False, dropout=0.0):\n",
    "    \"\"\"Generate U-Net model introduced in\n",
    "      \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"\n",
    "      O. Ronneberger, P. Fischer, T. Brox (2015)\n",
    "    Arbitrary number of input channels and output classes are supported.\n",
    "    Arguments:\n",
    "      height  - input image height (pixels)\n",
    "      width   - input image width  (pixels)\n",
    "      channels - input image features (1 for grayscale, 3 for RGB)\n",
    "      classes - number of output classes (2 in paper)\n",
    "      features - number of output features for first convolution (64 in paper)\n",
    "          Number of features double after each down sampling block\n",
    "      depth  - number of downsampling operations (4 in paper)\n",
    "      padding - 'valid' (used in paper) or 'same'\n",
    "      batchnorm - include batch normalization layers before activations\n",
    "      dropout - fraction of units to dropout, 0 to keep all units\n",
    "    Output:\n",
    "      U-Net model expecting input shape (height, width, maps) and generates\n",
    "      output with shape (output_height, output_width, classes). If padding is\n",
    "      'same', then output_height = height and output_width = width.\n",
    "    \"\"\"\n",
    "    x = Input(shape=(height, width, channels))\n",
    "    inputs = x\n",
    "\n",
    "    skips = []\n",
    "    for i in range(depth):\n",
    "        x, x0 = downsampling_block(x, features, padding,\n",
    "                                   batchnorm, dropout)\n",
    "        skips.append(x0)\n",
    "        features *= 2\n",
    "\n",
    "    x = Conv2D(filters=features, kernel_size=(3,3), padding=padding)(x)\n",
    "    x = BatchNormalization()(x) if batchnorm else x\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x) if dropout > 0 else x\n",
    "\n",
    "    x = Conv2D(filters=features, kernel_size=(3,3), padding=padding)(x)\n",
    "    x = BatchNormalization()(x) if batchnorm else x\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout)(x) if dropout > 0 else x\n",
    "\n",
    "    for i in reversed(range(depth)):\n",
    "        features //= 2\n",
    "        x = upsampling_block(x, skips[i], features, padding,\n",
    "                             batchnorm, dropout)\n",
    "\n",
    "    x = Conv2D(filters=classes, kernel_size=(1,1))(x)\n",
    "\n",
    "    logits = Lambda(lambda z: z/temperature)(x)\n",
    "    probabilities = Activation('softmax')(logits)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load original mri figures and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nrrd(full_path_filename):\n",
    "\n",
    "\tdata = sitk.ReadImage(full_path_filename)\n",
    "\tdata = sitk.Cast(sitk.RescaleIntensity(data),sitk.sitkUInt8)\n",
    "\tdata = sitk.GetArrayFromImage(data)\n",
    "\n",
    "\treturn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode mask \t\n",
    "def run_length_decoding(run_lengths,img):\n",
    "    \n",
    "    h,w = img.shape\n",
    "    mask = np.zeros(h*w)\n",
    "    if run_lengths == '[\\n]':\n",
    "        pass\n",
    "    else:\n",
    "        run_lengths_s = run_lengths[0].split()\n",
    "        #print(run_lengths_s)\n",
    "        for i in range(len(run_lengths_s)):\n",
    "            #even number is index and odd number is # of consecutive tags\n",
    "            if i%2 == 0:\n",
    "                #print(i)\n",
    "                mask[(int(run_lengths_s[i])-1):(int(run_lengths_s[i])+int(run_lengths_s[i+1])-1)] = 1\n",
    "        mask = mask.reshape((h,w)).T\n",
    "        \n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/Users/qinwenhuang/Documents/AtriaSeg_2018_training/train_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_labels = pickle.load(open(\"train_labels.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomShiftScaleRotate(image, mask,\n",
    "                           shift_limit=(-0.0625, 0.0625),\n",
    "                           scale_limit=(-0.1, 0.1),\n",
    "                           rotate_limit=(-45, 45), aspect_limit=(0, 0),\n",
    "                           borderMode=cv2.BORDER_CONSTANT, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        height, width, channel = image.shape\n",
    "\n",
    "        angle = np.random.uniform(rotate_limit[0], rotate_limit[1])  # degree\n",
    "        scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])\n",
    "        aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])\n",
    "        sx = scale * aspect / (aspect ** 0.5)\n",
    "        sy = scale / (aspect ** 0.5)\n",
    "        dx = round(np.random.uniform(shift_limit[0], shift_limit[1]) * width)\n",
    "        dy = round(np.random.uniform(shift_limit[0], shift_limit[1]) * height)\n",
    "\n",
    "        cc = np.math.cos(angle / 180 * np.math.pi) * sx\n",
    "        ss = np.math.sin(angle / 180 * np.math.pi) * sy\n",
    "        rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n",
    "\n",
    "        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])\n",
    "        box1 = box0 - np.array([width / 2, height / 2])\n",
    "        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])\n",
    "\n",
    "        box0 = box0.astype(np.float32)\n",
    "        box1 = box1.astype(np.float32)\n",
    "        mat = cv2.getPerspectiveTransform(box0, box1)\n",
    "        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                    borderValue=(\n",
    "                                        0, 0,\n",
    "                                        0,))\n",
    "        mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                   borderValue=(\n",
    "                                       0, 0,\n",
    "                                       0,))\n",
    "\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomHorizontalFlip(image, mask, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = cv2.flip(image, 1)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataLoading(object):\n",
    "    \"\"\"\n",
    "    data directory structure\n",
    "    TrainingSet/\n",
    "    each patient/\n",
    "    mask.nrrd, original.nrrd\n",
    "    \"\"\"\n",
    "    def __init__(self, directory, mask_dict):\n",
    "        self.directory = directory\n",
    "        self.mask_dict = mask_dict\n",
    "        self.mris = []\n",
    "        self.mri_names = []\n",
    "        self.masks = []\n",
    "        #self.load_images()\n",
    "        #self.load_masks()\n",
    "    def load_images(self):\n",
    "        \"\"\"\n",
    "        load all images from training set \n",
    "        go through subdirectories and get lgemri.nrrd, which represents patients original mri images\n",
    "        uses load_nrrd function \n",
    "        retun mri matrices and mri names\n",
    "        \"\"\"\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.directory, topdown = False):\n",
    "            for name in files:\n",
    "                if name == 'lgemri.nrrd':\n",
    "                    #print('yes')\n",
    "                    patient_name = root[-20:]+'_Slice_'\n",
    "                    full_name = os.path.join(root,name)\n",
    "                    single_patient_image = load_nrrd(full_name)\n",
    "                    num_of_slices = single_patient_image.shape[0]\n",
    "                    \n",
    "                    for i in range(num_of_slices):\n",
    "                        resze_single = cv2.resize(single_patient_image[i],(640,640))\n",
    "                        #resze_single = resze_single.reshape(resze_single,(640,640,1))\n",
    "                        #resze_single = np.expand_dims(resze_single, axis=2)\n",
    "                        self.mris.append(resze_single)\n",
    "                        self.mri_names.append(patient_name+str(i))\n",
    "        return self.mris, self.mri_names \n",
    "    def load_masks(self):\n",
    "        \"\"\"\n",
    "        covert all masks in rle to matrix format \n",
    "        return matrix format mask list\n",
    "        \"\"\"\n",
    "        #self.masks = []\n",
    "        for idx,name in enumerate(self.mri_names):\n",
    "            img = self.mris[idx]\n",
    "            encode_cav = train_labels[name]\n",
    "            #print(encode_cav)\n",
    "            output_mask = run_length_decoding(encode_cav,img)\n",
    "            resize_mask = cv2.resize(output_mask,(640,640))\n",
    "            self.masks.append(resize_mask)\n",
    "        return self.masks\n",
    "            \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, mask):\n",
    "    all_data = dataLoading(directory, mask)\n",
    "    image, image_name = all_data.load_images()\n",
    "    masks = all_data.load_masks()\n",
    "    return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topdir = '/home/qinwen/Documents/autoseg/Mini_Training'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different loss functions (dice, jaccard coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to compute dice coefficient\n",
    "def soft_sorensen_dice(y_true, y_pred, axis=None, smooth=1):\n",
    "    intersect = K.sum(y_true * y_pred, axis=axis)\n",
    "    area_true = K.sum(y_true, axis=axis)\n",
    "    area_pred = K.sum(y_pred, axis=axis)\n",
    "    return (2 * intersect + smooth) / (area_true + area_pred + smooth)\n",
    "    \n",
    "def hard_sorensen_dice(y_true, y_pred, axis=None, smooth=1):\n",
    "    y_true_int = K.round(y_true)\n",
    "    y_pred_int = K.round(y_pred)\n",
    "    return soft_sorensen_dice(y_true_int, y_pred_int, axis, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorensen_dice_loss(y_true, y_pred, weights):\n",
    "    # Input tensors have shape (batch_size, height, width, classes)\n",
    "    # must input list of weights with length equal to number of classes\n",
    "    #\n",
    "    # Ex: for simple binary classification, with the 0th mask\n",
    "    # corresponding to the background and the 1st mask corresponding\n",
    "    # to the object of interest, we set weights = [0, 1]\n",
    "    batch_dice_coefs = soft_sorensen_dice(y_true, y_pred, axis=[1, 2])\n",
    "    dice_coefs = K.mean(batch_dice_coefs, axis=0)\n",
    "    w = K.constant(weights) / sum(weights)\n",
    "    return 1 - K.sum(w * dice_coefs)\n",
    "\n",
    "def soft_jaccard(y_true, y_pred, axis=None, smooth=1):\n",
    "    intersect = K.sum(y_true * y_pred, axis=axis)\n",
    "    area_true = K.sum(y_true, axis=axis)\n",
    "    area_pred = K.sum(y_pred, axis=axis)\n",
    "    union = area_true + area_pred - intersection\n",
    "    return (intersect + smooth) / (union + smooth)\n",
    "\n",
    "def hard_jaccard(y_true, y_pred, axis=None, smooth=1):\n",
    "    y_true_int = K.round(y_true)\n",
    "    y_pred_int = K.round(y_pred)\n",
    "    return soft_jaccard(y_true_int, y_pred_int, axis, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_loss(y_true, y_pred, weights):\n",
    "    batch_jaccard_coefs = soft_jaccard(y_true, y_pred, axis=[1, 2])\n",
    "    jaccard_coefs = K.mean(batch_jaccard_coefs, axis=0)\n",
    "    w = K.constant(weights) / sum(weights)\n",
    "    return 1 - K.sum(w * jaccard_coefs)\n",
    "\n",
    "def weighted_categorical_crossentropy(y_true, y_pred, weights, epsilon=1e-8):\n",
    "    ndim = K.ndim(y_pred)\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    ncategory = K.int_shape(y_pred)[-1]\n",
    "    # scale predictions so class probabilities of each pixel sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=(ndim-1), keepdims=True)\n",
    "    y_pred = K.clip(y_pred, epsilon, 1-epsilon)\n",
    "    w = K.constant(weights) * (ncategory / sum(weights))\n",
    "    # first, average over all axis except classes\n",
    "    cross_entropies = -K.mean(y_true * K.log(y_pred), axis=tuple(range(ndim-1)))\n",
    "    return K.sum(w * cross_entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_transform(image, alpha, sigma, mode, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003].\n",
    "       Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\"\n",
    "    \"\"\"\n",
    "    assert len(image.shape)== 3\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    dx = gaussian_filter(2*random_state.rand(height, width) - 1,\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter(2*random_state.rand(height, width) - 1,\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "    indices = (np.repeat(np.ravel(x+dx), channels),\n",
    "               np.repeat(np.ravel(y+dy), channels),\n",
    "               np.tile(np.arange(channels), height*width))\n",
    "    \n",
    "    values = map_coordinates(image, indices, order=1, mode=mode)\n",
    "\n",
    "    return values.reshape((height, width, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create iterator for better data iteration - this is for loading data to Neural Nets  \n",
    "class Iterator(object):\n",
    "    def __init__(self, images, masks, batch_size,\n",
    "                 shuffle=True,\n",
    "                 rotation_range=90,\n",
    "                 width_shift_range=0.1,\n",
    "                 height_shift_range=0.1,\n",
    "                 shear_range=0.1,\n",
    "                 zoom_range=0.01,\n",
    "                 fill_mode='nearest',\n",
    "                 alpha=500,\n",
    "                 sigma=20):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        augment_options = {\n",
    "            'rotation_range': rotation_range,\n",
    "            'width_shift_range': width_shift_range,\n",
    "            'height_shift_range': height_shift_range,\n",
    "            'shear_range': shear_range,\n",
    "            'zoom_range': zoom_range,\n",
    "            'fill_mode': fill_mode,\n",
    "        }\n",
    "        self.idg = ImageDataGenerator(**augment_options)\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.fill_mode = fill_mode\n",
    "        self.i = 0\n",
    "        self.index = np.arange(len(images))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        # compute how many images to output in this batch\n",
    "        start = self.i\n",
    "        end = min(start + self.batch_size, len(self.images))\n",
    "        augmented_images = []\n",
    "        augmented_masks = []\n",
    "        for n in self.index[start:end]:\n",
    "            image = self.images[n]\n",
    "            mask = self.masks[n]\n",
    "            h,w,channels = image.shape\n",
    "            #h,w = image.shape\n",
    "\n",
    "            # stack image + mask together to simultaneously augment\n",
    "            stacked = np.concatenate((image, mask), axis=2)\n",
    "\n",
    "            # apply simple affine transforms first using Keras\n",
    "            augmented = self.idg.random_transform(stacked)\n",
    "\n",
    "            # maybe apply elastic deformation\n",
    "            if self.alpha != 0 and self.sigma != 0:\n",
    "                augmented = elastic_transform(\n",
    "                    augmented, self.alpha, self.sigma, self.fill_mode)\n",
    "\n",
    "            # split image and mask back apart\n",
    "            augmented_image = augmented[:,:,:channels]\n",
    "            augmented_images.append(augmented_image)\n",
    "            augmented_mask = np.round(augmented[:,:,channels:])\n",
    "            augmented_masks.append(augmented_mask)\n",
    "\n",
    "        self.i += self.batch_size\n",
    "        if self.i >= len(self.images):\n",
    "            self.i = 0\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.index)\n",
    "\n",
    "        return np.asarray(augmented_images), np.asarray(augmented_masks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, epsilon=1e-7, axis=1):\n",
    "    \n",
    "    x -= np.mean(x, axis=axis, keepdims=True)\n",
    "    x /= np.std(x, axis=axis, keepdims=True) + epsilon\n",
    "\n",
    "def create_generators(data_dir, batch_size, validation_split=0.0, mask=train_labels,\n",
    "                      shuffle_train_val=True, shuffle=True, seed=None,\n",
    "                      normalize_images=True, augment_training=False,\n",
    "                      augment_validation=False, augmentation_args={}):\n",
    "    images, masks = load_images(data_dir, mask)\n",
    "\n",
    "    # before: type(masks) = uint8 and type(images) = uint8\n",
    "    # convert images to double-precision\n",
    "    #images = images.astype('float64')\n",
    "    for i,img in enumerate(images):\n",
    "        images[i] = images[i].astype('float64')\n",
    "    # maybe normalize image\n",
    "    if normalize_images:\n",
    "        for i in images:\n",
    "            normalize(i, axis=1)\n",
    "    images = np.dstack(images)\n",
    "    images = np.rollaxis(images,-1)\n",
    "    masks = np.dstack(masks)\n",
    "    masks = np.rollaxis(masks, -1)\n",
    "    images = np.expand_dims(images, axis=3)\n",
    "    masks = np.expand_dims(masks, axis=3)\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if shuffle_train_val:\n",
    "        # shuffle images and masks in parallel\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(images)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(masks)\n",
    "\n",
    "    # split out last %(validation_split) of images as validation set\n",
    "    split_index = int((1-validation_split) * len(images))\n",
    "\n",
    "    if augment_training:\n",
    "        train_generator = Iterator(\n",
    "            images[:split_index], masks[:split_index],\n",
    "            batch_size, shuffle=shuffle, **augmentation_args)\n",
    "    else:\n",
    "        idg = ImageDataGenerator()\n",
    "        train_generator = idg.flow(images[:split_index], masks[:split_index],\n",
    "                                   batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    train_steps_per_epoch = np.ceil(split_index / batch_size)\n",
    "\n",
    "    if validation_split > 0.0:\n",
    "        if augment_validation:\n",
    "            val_generator = Iterator(\n",
    "                images[split_index:], masks[split_index:],\n",
    "                batch_size, shuffle=shuffle, **augmentation_args)\n",
    "        else:\n",
    "            idg = ImageDataGenerator()\n",
    "            val_generator = idg.flow(images[split_index:], masks[split_index:],\n",
    "                                     batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        val_generator = None\n",
    "\n",
    "    val_steps_per_epoch = np.ceil((len(images) - split_index) / batch_size)\n",
    "\n",
    "    return (train_generator, train_steps_per_epoch,\n",
    "            val_generator, val_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can choose to use different optimizers\n",
    "#this function takes in optimizer name and arguments for optimizer (e.g. learning rate, decay, momentum)\n",
    "def use_optimizer(name, args):\n",
    "    \"\"\"\n",
    "    7 possible optimizers - usually adam works the best\n",
    "    see paper - \"Adam, A Method for Stochastic Optimization\" by Kingma and Ba, arXiv 1412.6980\n",
    "    \"\"\"\n",
    "    optimizers = {\n",
    "        'sgd': SGD,\n",
    "        'rmsprop': RMSprop,\n",
    "        'adagrad': Adagrad,\n",
    "        'adadelta': Adadelta,\n",
    "        'adam': Adam,\n",
    "        'adamax': Adamax,\n",
    "        'nadam': Nadam,\n",
    "    }\n",
    "    if name not in optimizers:\n",
    "        raise Exception(\"Unknown optimizer ({}).\".format(name))\n",
    "\n",
    "    return optimizers[name](**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use train function to train the neural nets - if you want to train the net, just run this function with all arguments\n",
    "def train(rotation_range, width_shift_rage, height_shift_range, shear_range, zoom_range, fill_mode, datadir, batch_size, validation_split, learning_rate, decay,optimizer, loss, loss_weights):\n",
    "\n",
    "    augmentation_args = {\n",
    "        'rotation_range': rotation_range,\n",
    "        'width_shift_range': width_shift_range,\n",
    "        'height_shift_range': height_shift_range,\n",
    "        'shear_range': shear_range,\n",
    "        'zoom_range': zoom_range,\n",
    "        'fill_mode' : fill_mode,\n",
    "\n",
    "    }\n",
    "    train_generator, train_steps_per_epoch, val_generator, val_steps_per_epoch = create_generators(\n",
    "            datadir, batch_size,\n",
    "            validation_split=validation_split,\n",
    "            shuffle_train_val=shuffle_train_val,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            normalize_images=normalize,\n",
    "            augment_training=augment_training,\n",
    "            augment_validation=augment_validation,\n",
    "            augmentation_args=augmentation_args)\n",
    "    # get image dimensions from first batch\n",
    "    images, masks = next(train_generator)\n",
    "    _,height,width,channels = images.shape\n",
    "    _,_,_,classes = masks.shape\n",
    "    #start building model\n",
    "    model = unet(height=height, width=width, channels=channels, classes=classes,dropout = 0.5)\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    optimizer_args = {\n",
    "        'lr':       learning_rate,\n",
    "        'decay':    decay\n",
    "    }\n",
    "    for k in list(optimizer_args):\n",
    "        if optimizer_args[k] is None:\n",
    "            del optimizer_args[k]\n",
    "    optimizer = use_optimizer(optimizer, optimizer_args)\n",
    "    \n",
    "    if loss == 'pixel':\n",
    "        def lossfunc(y_true, y_pred):\n",
    "            return weighted_categorical_crossentropy(\n",
    "                y_true, y_pred, loss_weights)\n",
    "    elif loss == 'dice':\n",
    "        def lossfunc(y_true, y_pred):\n",
    "            return sorensen_dice_loss(y_true, y_pred, loss_weights)\n",
    "    elif loss == 'jaccard':\n",
    "        def lossfunc(y_true, y_pred):\n",
    "            return jaccard_loss(y_true, y_pred, loss_weights)\n",
    "    else:\n",
    "        raise Exception(\"Unknown loss ({})\".format(args.loss))\n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy')\n",
    "    model.fit_generator(generator=train_generator,validation_data = val_generator, steps_per_epoch = train_steps_per_epoch, validation_steps = val_steps_per_epoch, use_multiprocessing=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotation_range, width_shift_rage, height_shift_range, shear_range, zoom_range, fill_mode, datadir, batch_size, validation_split, learning_rate, momentum, decay,optimizer, loss, loss_weights\n",
    "rotation_range = 90\n",
    "width_shift_range = 0.1\n",
    "height_shift_range=0.1\n",
    "shear_range = 0.1\n",
    "zoom_range = 0.01\n",
    "fill_mode='nearest'\n",
    "datadir = topdir\n",
    "batch_size = 16\n",
    "validation_split = 0.2\n",
    "learning_rate = 0.01\n",
    "#momentum = 0.01\n",
    "decay = 0.001\n",
    "optimizer = 'adam'\n",
    "loss = 'pixel'\n",
    "loss_weights = 0.1\n",
    "shuffle_train_val = True\n",
    "shuffle = True\n",
    "seed = None\n",
    "augment_training=True\n",
    "augment_validation=True\n",
    "augment_options = {\n",
    "            'rotation_range': rotation_range,\n",
    "            'width_shift_range': width_shift_range,\n",
    "            'height_shift_range': height_shift_range,\n",
    "            'shear_range': shear_range,\n",
    "            'zoom_range': zoom_range,\n",
    "            'fill_mode': fill_mode,\n",
    "        }\n",
    "augmentation_args = augment_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 640, 640, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 640, 640, 64) 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 640, 640, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 640, 640, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 640, 640, 64) 36928       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 640, 640, 64) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 640, 640, 64) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 320, 320, 64) 0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 320, 320, 128 73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 320, 320, 128 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 320, 320, 128 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 320, 320, 128 147584      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 320, 320, 128 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 320, 320, 128 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 160, 160, 128 0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 160, 160, 256 295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 160, 160, 256 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 160, 160, 256 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 160, 160, 256 590080      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 160, 160, 256 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 160, 160, 256 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 80, 80, 256)  0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 80, 80, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 80, 80, 512)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 80, 80, 512)  0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 80, 80, 512)  2359808     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 80, 80, 512)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 80, 80, 512)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 40, 40, 512)  0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 40, 40, 1024) 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 40, 40, 1024) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 40, 40, 1024) 0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 40, 40, 1024) 9438208     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 40, 40, 1024) 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 40, 40, 1024) 0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 80, 80, 512)  2097664     dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80, 80, 1024) 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 80, 80, 512)  4719104     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 80, 80, 512)  0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 80, 80, 512)  0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 80, 80, 512)  2359808     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 80, 80, 512)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 80, 80, 512)  0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 160, 160, 256 524544      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 160, 160, 512 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 160, 160, 256 1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 160, 160, 256 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 160, 160, 256 0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 160, 160, 256 590080      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 160, 160, 256 0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 160, 160, 256 0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 320, 320, 128 131200      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 320, 320, 256 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 320, 320, 128 295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 320, 320, 128 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 320, 320, 128 0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 320, 320, 128 147584      dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 320, 320, 128 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 320, 320, 128 0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 640, 640, 64) 32832       dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 640, 640, 128 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 640, 640, 64) 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 640, 640, 64) 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 640, 640, 64) 0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 640, 640, 64) 36928       dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 640, 640, 64) 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 640, 640, 64) 0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 640, 640, 1)  65          dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 640, 640, 1)  0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 640, 640, 1)  0           lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 31,030,593\n",
      "Trainable params: 31,030,593\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[16,640,640,64]\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=2385188, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dropout_1/cond/dropout/Shape)]]\n\t [[Node: loss/mul/_465 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1313_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'dropout_1/cond/dropout/random_uniform/RandomUniform', defined at:\n  File \"/home/qinwen/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/qinwen/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/qinwen/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/qinwen/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/qinwen/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-601758539595>\", line 1, in <module>\n    train(rotation_range, width_shift_range, height_shift_range, shear_range, zoom_range, fill_mode, datadir, batch_size, validation_split, learning_rate, decay,optimizer, loss, loss_weights)\n  File \"<ipython-input-28-3b0c5f7a34bf>\", line 28, in train\n    model = unet(height=height, width=width, channels=channels, classes=classes,dropout = 0.5)\n  File \"<ipython-input-7-65e512849558>\", line 78, in unet\n    batchnorm, dropout)\n  File \"<ipython-input-7-65e512849558>\", line 10, in downsampling_block\n    x = Dropout(dropout)(x) if dropout > 0 else x\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 460, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 125, in call\n    training=training)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3069, in in_train_phase\n    x = switch(training, x, alt)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3004, in switch\n    else_expression_fn)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1738, in cond\n    orig_res, res_t = context_t.BuildCondBranch(fn1)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1639, in BuildCondBranch\n    r = fn()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 123, in dropped_inputs\n    seed=self.seed)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3363, in dropout\n    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1936, in dropout\n    dtype=x.dtype)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 244, in random_uniform\n    seed2=seed2)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 220, in _random_uniform\n    seed=seed, seed2=seed2, name=name)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,640,640,64]\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=2385188, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dropout_1/cond/dropout/Shape)]]\n\t [[Node: loss/mul/_465 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1313_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,640,640,64]\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=2385188, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dropout_1/cond/dropout/Shape)]]\n\t [[Node: loss/mul/_465 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1313_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-601758539595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_shift_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_shift_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshear_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-3b0c5f7a34bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rotation_range, width_shift_rage, height_shift_range, shear_range, zoom_range, fill_mode, datadir, batch_size, validation_split, learning_rate, decay, optimizer, loss, loss_weights)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown loss ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_steps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2649\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,640,640,64]\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=2385188, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dropout_1/cond/dropout/Shape)]]\n\t [[Node: loss/mul/_465 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1313_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'dropout_1/cond/dropout/random_uniform/RandomUniform', defined at:\n  File \"/home/qinwen/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/qinwen/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/qinwen/anaconda3/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/qinwen/anaconda3/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/qinwen/anaconda3/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-601758539595>\", line 1, in <module>\n    train(rotation_range, width_shift_range, height_shift_range, shear_range, zoom_range, fill_mode, datadir, batch_size, validation_split, learning_rate, decay,optimizer, loss, loss_weights)\n  File \"<ipython-input-28-3b0c5f7a34bf>\", line 28, in train\n    model = unet(height=height, width=width, channels=channels, classes=classes,dropout = 0.5)\n  File \"<ipython-input-7-65e512849558>\", line 78, in unet\n    batchnorm, dropout)\n  File \"<ipython-input-7-65e512849558>\", line 10, in downsampling_block\n    x = Dropout(dropout)(x) if dropout > 0 else x\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 460, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 125, in call\n    training=training)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3069, in in_train_phase\n    x = switch(training, x, alt)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3004, in switch\n    else_expression_fn)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1738, in cond\n    orig_res, res_t = context_t.BuildCondBranch(fn1)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1639, in BuildCondBranch\n    r = fn()\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 123, in dropped_inputs\n    seed=self.seed)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3363, in dropout\n    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1936, in dropout\n    dtype=x.dtype)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 244, in random_uniform\n    seed2=seed2)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 220, in _random_uniform\n    seed=seed, seed2=seed2, name=name)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/qinwen/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[16,640,640,64]\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=2385188, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](dropout_1/cond/dropout/Shape)]]\n\t [[Node: loss/mul/_465 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1313_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "train(rotation_range, width_shift_range, height_shift_range, shear_range, zoom_range, fill_mode, datadir, batch_size, validation_split, learning_rate, decay,optimizer, loss, loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute statistics in each round \n",
    "def compute_statistics(model, generator, steps_per_epoch, return_images=False):\n",
    "    dices = []\n",
    "    jaccards = []\n",
    "    predictions = []\n",
    "    for i in range(steps_per_epoch):\n",
    "        images, masks_true = next(generator)\n",
    "        # Normally: masks_pred = model.predict(images)\n",
    "        # But dilated densenet cannot handle large batch size\n",
    "        masks_pred = np.concatenate([model.predict(image[None,:,:,:]) for image in images])\n",
    "        for mask_true, mask_pred in zip(masks_true, masks_pred):\n",
    "            y_true = mask_true[:,:,1].astype('uint8')\n",
    "            y_pred = np.round(mask_pred[:,:,1]).astype('uint8')\n",
    "            dices.append(sorensen_dice(y_true, y_pred))\n",
    "            jaccards.append(jaccard(y_true, y_pred))\n",
    "        if return_images:\n",
    "            for image, mask_true, mask_pred in zip(images, masks_true, masks_pred):\n",
    "                predictions.append((image[:,:,0], mask_true[:,:,1], mask_pred[:,:,1]))\n",
    "    print(\"Dice:    {:.3f} ({:.3f})\".format(np.mean(dices), np.std(dices)))\n",
    "    print(\"Jaccard: {:.3f} ({:.3f})\".format(np.mean(jaccards), np.std(jaccards)))\n",
    "    return dices, jaccards, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
